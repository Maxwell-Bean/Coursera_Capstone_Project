Classification with Python

In this notebook we try to practice all the classification algorithms
that we learned in this course.

We load a dataset using Pandas library, and apply the following
algorithms, and find the best one for this specific dataset by accuracy
evaluation methods.

Lets first load required libraries:


+*In[1]:*+
[source, ipython3]
----
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline
----

== About dataset

This dataset is about past loans. The *Loan_train.csv* data set includes
details of 346 customers whose loan are already paid off or defaulted.
It includes following fields:

[width="100%",cols="16%,84%",options="header",]
|===
|Field |Description
|Loan_status |Whether a loan is paid off on in collection

|Principal |Basic principal loan amount at the

|Terms |Origination terms which can be weekly (7 days), biweekly, and
monthly payoff schedule

|Effective_date |When the loan got originated and took effects

|Due_date |Since it’s one-time payoff schedule, each loan has one single
due date

|Age |Age of applicant

|Education |Education of applicant

|Gender |The gender of applicant
|===

Lets download the dataset


+*In[2]:*+
[source, ipython3]
----
!wget -O loan_train.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv
----


+*Out[2]:*+
----
--2020-10-22 21:09:16--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv
Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196
Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 23101 (23K) [text/csv]
Saving to: ‘loan_train.csv’

loan_train.csv      100%[===================>]  22.56K  --.-KB/s    in 0.002s  

2020-10-22 21:09:18 (13.4 MB/s) - ‘loan_train.csv’ saved [23101/23101]

----

== Load Data From CSV File


+*In[3]:*+
[source, ipython3]
----
df = pd.read_csv('loan_train.csv')
df.head()
----


+*Out[3]:*+
----
[cols=",,,,,,,,,,",options="header",]
|===
| |Unnamed: 0 |Unnamed: 0.1 |loan_status |Principal |terms
|effective_date |due_date |age |education |Gender
|0 |0 |0 |PAIDOFF |1000 |30 |9/8/2016 |10/7/2016 |45 |High School or
Below |male

|1 |2 |2 |PAIDOFF |1000 |30 |9/8/2016 |10/7/2016 |33 |Bechalor |female

|2 |3 |3 |PAIDOFF |1000 |15 |9/8/2016 |9/22/2016 |27 |college |male

|3 |4 |4 |PAIDOFF |1000 |30 |9/9/2016 |10/8/2016 |28 |college |female

|4 |6 |6 |PAIDOFF |1000 |30 |9/9/2016 |10/8/2016 |29 |college |male
|===
----


+*In[4]:*+
[source, ipython3]
----
df.shape
----


+*Out[4]:*+
----(346, 10)----

== Convert to date time object


+*In[5]:*+
[source, ipython3]
----
df['due_date'] = pd.to_datetime(df['due_date'])
df['effective_date'] = pd.to_datetime(df['effective_date'])
df.head()
----


+*Out[5]:*+
----
[cols=",,,,,,,,,,",options="header",]
|===
| |Unnamed: 0 |Unnamed: 0.1 |loan_status |Principal |terms
|effective_date |due_date |age |education |Gender
|0 |0 |0 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |45 |High School or
Below |male

|1 |2 |2 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |33 |Bechalor
|female

|2 |3 |3 |PAIDOFF |1000 |15 |2016-09-08 |2016-09-22 |27 |college |male

|3 |4 |4 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |28 |college |female

|4 |6 |6 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |29 |college |male
|===
----

== Data visualization and pre-processing

Let’s see how many of each class is in our data set


+*In[6]:*+
[source, ipython3]
----
df['loan_status'].value_counts()
----


+*Out[6]:*+
----PAIDOFF       260
COLLECTION     86
Name: loan_status, dtype: int64----

260 people have paid off the loan on time while 86 have gone into
collection

Lets plot some columns to underestand data better:


+*In[7]:*+
[source, ipython3]
----
# notice: installing seaborn might takes a few minutes
#!conda install -c anaconda seaborn -y
----


+*In[8]:*+
[source, ipython3]
----
import seaborn as sns

bins = np.linspace(df.Principal.min(), df.Principal.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'Principal', bins=bins, ec="k")

g.axes[-1].legend()
plt.show()
----


+*Out[8]:*+
----
![png](output_18_0.png)
----


+*In[9]:*+
[source, ipython3]
----
bins = np.linspace(df.age.min(), df.age.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'age', bins=bins, ec="k")

g.axes[-1].legend()
plt.show()
----


+*Out[9]:*+
----
![png](output_19_0.png)
----

== Pre-processing: Feature selection/extraction

== Lets look at the day of the week people get the loan


+*In[10]:*+
[source, ipython3]
----
df['dayofweek'] = df['effective_date'].dt.dayofweek
bins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
g.map(plt.hist, 'dayofweek', bins=bins, ec="k")
g.axes[-1].legend()
plt.show()

----


+*Out[10]:*+
----
![png](output_22_0.png)
----

We see that people who get the loan at the end of the week dont pay it
off, so lets use Feature binarization to set a threshold values less
then day 4


+*In[11]:*+
[source, ipython3]
----
df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
df.head()
----


+*Out[11]:*+
----
[cols=",,,,,,,,,,,,",options="header",]
|===
| |Unnamed: 0 |Unnamed: 0.1 |loan_status |Principal |terms
|effective_date |due_date |age |education |Gender |dayofweek |weekend
|0 |0 |0 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |45 |High School or
Below |male |3 |0

|1 |2 |2 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |33 |Bechalor
|female |3 |0

|2 |3 |3 |PAIDOFF |1000 |15 |2016-09-08 |2016-09-22 |27 |college |male
|3 |0

|3 |4 |4 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |28 |college |female
|4 |1

|4 |6 |6 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |29 |college |male
|4 |1
|===
----

== Convert Categorical features to numerical values

Lets look at gender:


+*In[12]:*+
[source, ipython3]
----
df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
----


+*Out[12]:*+
----Gender  loan_status
female  PAIDOFF        0.865385
        COLLECTION     0.134615
male    PAIDOFF        0.731293
        COLLECTION     0.268707
Name: loan_status, dtype: float64----

86 % of female pay there loans while only 73 % of males pay there loan

Lets convert male to 0 and female to 1:


+*In[13]:*+
[source, ipython3]
----
df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
df.head()
----


+*Out[13]:*+
----
[cols=",,,,,,,,,,,,",options="header",]
|===
| |Unnamed: 0 |Unnamed: 0.1 |loan_status |Principal |terms
|effective_date |due_date |age |education |Gender |dayofweek |weekend
|0 |0 |0 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |45 |High School or
Below |0 |3 |0

|1 |2 |2 |PAIDOFF |1000 |30 |2016-09-08 |2016-10-07 |33 |Bechalor |1 |3
|0

|2 |3 |3 |PAIDOFF |1000 |15 |2016-09-08 |2016-09-22 |27 |college |0 |3
|0

|3 |4 |4 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |28 |college |1 |4
|1

|4 |6 |6 |PAIDOFF |1000 |30 |2016-09-09 |2016-10-08 |29 |college |0 |4
|1
|===
----

== One Hot Encoding

=== How about education?


+*In[14]:*+
[source, ipython3]
----
df.groupby(['education'])['loan_status'].value_counts(normalize=True)
----


+*Out[14]:*+
----education             loan_status
Bechalor              PAIDOFF        0.750000
                      COLLECTION     0.250000
High School or Below  PAIDOFF        0.741722
                      COLLECTION     0.258278
Master or Above       COLLECTION     0.500000
                      PAIDOFF        0.500000
college               PAIDOFF        0.765101
                      COLLECTION     0.234899
Name: loan_status, dtype: float64----

== Feature befor One Hot Encoding


+*In[15]:*+
[source, ipython3]
----
df[['Principal','terms','age','Gender','education']].head()
----


+*Out[15]:*+
----
[cols=",,,,,",options="header",]
|===
| |Principal |terms |age |Gender |education
|0 |1000 |30 |45 |0 |High School or Below
|1 |1000 |30 |33 |1 |Bechalor
|2 |1000 |15 |27 |0 |college
|3 |1000 |30 |28 |1 |college
|4 |1000 |30 |29 |0 |college
|===
----

== Use one hot encoding technique to conver categorical varables to binary variables and append them to the feature Data Frame


+*In[16]:*+
[source, ipython3]
----
Feature = df[['Principal','terms','age','Gender','weekend']]
Feature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)
Feature.drop(['Master or Above'], axis = 1,inplace=True)
Feature.head()

----


+*Out[16]:*+
----
[cols=",,,,,,,,",options="header",]
|===
| |Principal |terms |age |Gender |weekend |Bechalor |High School or
Below |college
|0 |1000 |30 |45 |0 |0 |0 |1 |0

|1 |1000 |30 |33 |1 |0 |1 |0 |0

|2 |1000 |15 |27 |0 |0 |0 |0 |1

|3 |1000 |30 |28 |1 |1 |0 |0 |1

|4 |1000 |30 |29 |0 |1 |0 |0 |1
|===
----

== Feature selection

Lets defind feature sets, X:


+*In[17]:*+
[source, ipython3]
----
X = Feature
X[0:5]
----


+*Out[17]:*+
----
[cols=",,,,,,,,",options="header",]
|===
| |Principal |terms |age |Gender |weekend |Bechalor |High School or
Below |college
|0 |1000 |30 |45 |0 |0 |0 |1 |0

|1 |1000 |30 |33 |1 |0 |1 |0 |0

|2 |1000 |15 |27 |0 |0 |0 |0 |1

|3 |1000 |30 |28 |1 |1 |0 |0 |1

|4 |1000 |30 |29 |0 |1 |0 |0 |1
|===
----

What are our lables?


+*In[18]:*+
[source, ipython3]
----
#y for collection and paidoff, y2 for 0 and 1
y = df['loan_status'].values

----

== Normalize Data

Data Standardization give data zero mean and unit variance (technically
should be done after train test split )


+*In[19]:*+
[source, ipython3]
----
X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
----


+*Out[19]:*+
----array([[ 0.51578458,  0.92071769,  2.33152555, -0.42056004, -1.20577805,
        -0.38170062,  1.13639374, -0.86968108],
       [ 0.51578458,  0.92071769,  0.34170148,  2.37778177, -1.20577805,
         2.61985426, -0.87997669, -0.86968108],
       [ 0.51578458, -0.95911111, -0.65321055, -0.42056004, -1.20577805,
        -0.38170062, -0.87997669,  1.14984679],
       [ 0.51578458,  0.92071769, -0.48739188,  2.37778177,  0.82934003,
        -0.38170062, -0.87997669,  1.14984679],
       [ 0.51578458,  0.92071769, -0.3215732 , -0.42056004,  0.82934003,
        -0.38170062, -0.87997669,  1.14984679]])----

== Classification

Now, it is your turn, use the training set to build an accurate model.
Then use the test set to report the accuracy of the model You should use
the following algorithm: - K Nearest Neighbor(KNN) - Decision Tree -
Support Vector Machine - Logistic Regression

__ Notice:__ - You can go above and change the pre-processing, feature
selection, feature-extraction, and so on, to make a better model. - You
should use either scikit-learn, Scipy or Numpy libraries for developing
the classification algorithms. - You should include the code of the
algorithm in the following cells.

== K Nearest Neighbor(KNN)

Notice: You should find the best k to build the model with the best
accuracy. +
*warning:* You should not use the *loan_test.csv* for finding the best
k, however, you can split your train_loan.csv into train and test to
find the best *k*.


+*In[20]:*+
[source, ipython3]
----
# Import required libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.156, random_state=4)
#print ('Train set:', X_train.shape,  y_train.shape)
#print ('Test set:', X_test.shape,  y_test.shape)
----


+*In[21]:*+
[source, ipython3]
----
# Start the algorithm
k = 5
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
yhatk = neigh.predict(X_test)
#yhatk.shape
----


+*In[22]:*+
[source, ipython3]
----
#find optimal k
Ks=15
mean_acc=np.zeros((Ks-1))
std_acc=np.zeros((Ks-1))
ConfustionMx=[];
for n in range(1,Ks):
    
    #Train Model and Predict  
    kNN_model = KNeighborsClassifier(n_neighbors=n).fit(X_train,y_train)
    yhat = kNN_model.predict(X_test)
    
    
    mean_acc[n-1]=np.mean(yhat==y_test);
    
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])
mean_acc
----


+*Out[22]:*+
----array([0.66666667, 0.62962963, 0.72222222, 0.66666667, 0.72222222,
       0.68518519, 0.75925926, 0.72222222, 0.77777778, 0.66666667,
       0.72222222, 0.7037037 , 0.74074074, 0.68518519])----


+*In[23]:*+
[source, ipython3]
----
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()
----


+*Out[23]:*+
----
![png](output_51_0.png)
----


+*In[24]:*+
[source, ipython3]
----
# Building the model again, using k=7
from sklearn.neighbors import KNeighborsClassifier
k = 7
#Train Model and Predict  
kNN_model = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)
yhattk=kNN_model.predict(X_test)
#print("Jaccard Score is",jaccard_score(y_test, yhattk))
----

== Decision Tree


+*In[25]:*+
[source, ipython3]
----
# Import necessary package
from sklearn.tree import DecisionTreeClassifier
loanTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
loanTree
loanTree.fit(X,y)
----


+*Out[25]:*+
----DecisionTreeClassifier(criterion='entropy', max_depth=4)----


+*In[26]:*+
[source, ipython3]
----
#yhatd = loanTree.predict(X)
----


+*In[27]:*+
[source, ipython3]
----
#print (yhatd [0:5])
#print (y [0:5])

#from sklearn import metrics
#import matplotlib.pyplot as plt
#print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y, yhatd))
----

== Support Vector Machine


+*In[28]:*+
[source, ipython3]
----
from sklearn import svm
clf = svm.SVC(kernel='linear')
clf.fit(X, y) 
----


+*Out[28]:*+
----SVC(kernel='linear')----


+*In[29]:*+
[source, ipython3]
----
#yhatsv = clf.predict(X_test)
#yhatsv.shape
----


+*In[30]:*+
[source, ipython3]
----
#from sklearn.metrics import classification_report, confusion_matrix
#import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
----


+*In[31]:*+
[source, ipython3]
----
# Compute confusion matrix
#cnf_matrix = confusion_matrix(y, yhatsv, labels=[2,4])
#np.set_printoptions(precision=2)

#print (classification_report(y, yhatsv))

# Plot non-normalized confusion matrix
#plt.figure()
#plot_confusion_matrix(cnf_matrix, classes=['0','1'],normalize= False,  title='Confusion matrix')
----


+*In[32]:*+
[source, ipython3]
----
#from sklearn.metrics import f1_score
#f1_score(y, yhatsv, average='weighted')
#jaccard_score(y_test, yhat)
----

== Logistic Regression


+*In[33]:*+
[source, ipython3]
----
from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import confusion_matrix
----


+*In[34]:*+
[source, ipython3]
----

y[y=='PAIDOFF']=1
y[y=='COLLECTION']=0
y=y.astype('int')

y_test[y_test=='PAIDOFF']=1
y_test[y_test=='COLLECTION']=0
y_test=y_test.astype('int')

y_train[y_train=='PAIDOFF']=1
y_train[y_train=='COLLECTION']=0
y_train=y_train.astype('int')
LR2 = LogisticRegression(C=0.01, solver='saga').fit(X_train,y_train)
yhatlrt = LR2.predict(X_test)

LR = LogisticRegression(C=.5, solver='lbfgs').fit(X,y)

----


+*In[ ]:*+
[source, ipython3]
----

----

== Model Evaluation using Test set


+*In[35]:*+
[source, ipython3]
----
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
----

First, download and load the test set:


+*In[36]:*+
[source, ipython3]
----
!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv
----


+*Out[36]:*+
----
--2020-10-22 21:09:21--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv
Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196
Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3642 (3.6K) [text/csv]
Saving to: ‘loan_test.csv’

loan_test.csv       100%[===================>]   3.56K  --.-KB/s    in 0s      

2020-10-22 21:09:21 (37.1 MB/s) - ‘loan_test.csv’ saved [3642/3642]

----

== Load Test set for evaluation


+*In[37]:*+
[source, ipython3]
----
# Prep and clean the dataset part 1
test_df = pd.read_csv('loan_test.csv')
test_df['due_date'] = pd.to_datetime(test_df['due_date'])
test_df['effective_date'] = pd.to_datetime(test_df['effective_date'])
test_df['dayofweek'] = test_df['effective_date'].dt.dayofweek
test_df['weekend'] = test_df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
test_df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
test_df.head()
test_df.shape
----


+*Out[37]:*+
----(54, 12)----


+*In[38]:*+
[source, ipython3]
----
# Prep and clean the data set, part 2
test_df[['Principal','terms','age','Gender','education']].head()
Feature2 = test_df[['Principal','terms','age','Gender','weekend']]
Feature2 = pd.concat([Feature2,pd.get_dummies(test_df['education'])], axis=1)
Feature2.drop(['Master or Above'], axis = 1,inplace=True)

XTe=Feature2
XTe= preprocessing.StandardScaler().fit(XTe).transform(XTe)
yte = test_df['loan_status'].values

----


+*In[39]:*+
[source, ipython3]
----
# Jaccard for k nearest neighbor
print("K Nearest Neighbor")
print("Jaccard Score is",jaccard_score(yte, yhattk,pos_label='PAIDOFF'))
----


+*Out[39]:*+
----
K Nearest Neighbor
Jaccard Score is 0.6458333333333334
----


+*In[40]:*+
[source, ipython3]
----
# f1 score for k nearest neighbor
print("F1 score is",f1_score(yte, yhattk, average='weighted'))

----


+*Out[40]:*+
----
F1 score is 0.6886205279919816
----


+*In[41]:*+
[source, ipython3]
----
# Jaccard for decision tree
print("Decision Tree")
yhatd2 = loanTree.predict(XTe)
print("The Jaccard Score is", jaccard_score(yte, yhatd2,pos_label='PAIDOFF'))
----


+*Out[41]:*+
----
Decision Tree
The Jaccard Score is 0.7647058823529411
----


+*In[42]:*+
[source, ipython3]
----
# f1 score for decision tree
print("The F1 Score is", f1_score(yte, yhatd2, average='weighted')) 

----


+*Out[42]:*+
----
The F1 Score is 0.7283950617283951
----


+*In[43]:*+
[source, ipython3]
----
print("SVM")
yhatsv2 = clf.predict(XTe)
# Jaccard for SVM
print("The Jaccard Score is", jaccard_score(yte, yhatsv2,pos_label='PAIDOFF'))

----


+*Out[43]:*+
----
SVM
The Jaccard Score is 0.7407407407407407
----


+*In[44]:*+
[source, ipython3]
----
# f1 score for SVM
print("The F1 Score is",f1_score(yte, yhatsv2, average='weighted'))
----


+*Out[44]:*+
----
The F1 Score is 0.6304176516942475
----


+*In[45]:*+
[source, ipython3]
----
print("Logistic Regression")
yhatlr = LR.predict(XTe)
yte[yte=='PAIDOFF']=1
yte[yte=='COLLECTION']=0
yte=yte.astype('int')
# Jaccard for LogisticsRegression
print("The Jaccard Score is", jaccard_score(yte, yhatlr,average='weighted'))

----


+*Out[45]:*+
----
Logistic Regression
The Jaccard Score is 0.5486968449931412
----


+*In[46]:*+
[source, ipython3]
----
print("The F1 Score is", f1_score(yte, yhatlr, average='weighted'))
----


+*Out[46]:*+
----
The F1 Score is 0.6304176516942475
----


+*In[47]:*+
[source, ipython3]
----
print("The logloss is",log_loss(yte, yhatlr))

----


+*Out[47]:*+
----
The logloss is 8.95470488690319
----

== Report

You should be able to report the accuracy of the built model using
different evaluation metrics:

[cols=",,,",options="header",]
|===
|Algorithm |Jaccard |F1-score |LogLoss
|KNN |0.646 |0.689 |NA
|Decision Tree |0.765 |0.728 |NA
|SVM |0.741 |0.630 |NA
|LogisticRegression |0.549 |0.630 |8.955
|===

Want to learn more?

IBM SPSS Modeler is a comprehensive analytics platform that has many
machine learning algorithms. It has been designed to bring predictive
intelligence to decisions made by individuals, by groups, by systems –
by your enterprise as a whole. A free trial is available through this
course, available here: SPSS Modeler

Also, you can use Watson Studio to run these notebooks faster with
bigger datasets. Watson Studio is IBM’s leading cloud solution for data
scientists, built by data scientists. With Jupyter notebooks, RStudio,
Apache Spark and popular libraries pre-packaged in the cloud, Watson
Studio enables data scientists to collaborate on their projects without
having to install anything. Join the fast-growing community of Watson
Studio users today with a free account at Watson Studio

Thanks for completing this lesson!

Author: Saeed Aghabozorgi

Saeed Aghabozorgi, PhD is a Data Scientist in IBM with a track record of
developing enterprise level applications that substantially increases
clients’ ability to turn data into actionable knowledge. He is a
researcher in data mining field and expert in developing advanced
analytic methods like machine learning and statistical modelling on
large datasets.

Copyright © 2018 Cognitive Class. This notebook and its source code are
released under the terms of the MIT License.
